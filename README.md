## **INTRODUCTION**


# **1.Computational Graph**
> Write by Quang Van

Äá»“ thá»‹ tÃ­nh toÃ¡n(Computational Graph) lÃ  má»™t cÃ¡ch hay khi nÃ³i vá» biá»ƒu thá»©c tÃ­nh toÃ¡n.

    XÃ©t biá»ƒu thá»©c e = (a + b) âˆ— (b + 1)

á» Ä‘Ã¢y cÃ³ 3 toÃ¡n tá»­: 2(+) vÃ  1(*).  <br/>
Ta sá»­ dá»¥ng nhá»¯ng biáº¿n trung gian Ä‘á»ƒ biá»ƒu diá»…n láº¡i biá»ƒu thá»©c c=a+b, d=b+1 

                  e = c * d

Táº¡o 1 Ä‘á»“ thá»‹ tÃ­nh toÃ¡n, theo cÃ¡c biáº¿n input(a,b) tá»›i cÃ¡c node vÃ  giÃ¡ trá»‹ má»™t nÃºt lÃ  Ä‘áº§u vÃ o cá»§a nÃºt khÃ¡c. Äáº·t a = 2 vÃ  b = 1. Káº¿t quáº£ e = 6


![](https://i.imgur.com/urjvbKk.png)


---

Ta thá»­ Ä‘áº·t cÃ¢u há»i: "Náº¿u a áº£nh hÆ°á»Ÿng trá»±c tiáº¿p c, vÃ  a thay Ä‘á»•i thÃ¬ liá»‡u c cÃ³ thay Ä‘á»•i vÃ  thay Ä‘á»•i nhÆ° tháº¿ nÃ o?"

VÃ  chÃºng ta gá»i Ä‘Ã¢y lÃ  **Ä‘áº¡o hÃ m riÃªng** cá»§a c vá»›i Ä‘á»‘i vá»›i a ğŸ˜œ 
<br/>
<br/>
Ta quan sÃ¡t hÃ¬nh sau:


![](https://i.imgur.com/s42WyKE.png)

CÃ¡c báº¡n Ä‘á»ƒ Ã½ náº¿u a thay Ä‘á»•i 1 Ä‘Æ¡n vá»‹, vá»›i Ä‘iá»u kiá»‡n b Ä‘Æ°á»£c giá»¯ nguyÃªn thÃ¬ c thay Ä‘á»•i 1 Ä‘Æ¡n vá»‹. ÄÃ¢y chÃ­nh lÃ  hÃ¬nh áº£nh cá»§a Ä‘áº¡o hÃ m riÃªng ğŸ™‚ <br/>
Äáº¿n lÆ°á»£t c thay Ä‘á»•i 1 Ä‘Æ¡n vá»‹, vá»›i Ä‘iá»u kiá»‡n d Ä‘Æ°á»£c giá»¯ nguyÃªn thÃ¬ khiáº¿n e thay Ä‘á»•i 1*2 Ä‘Æ¡n vá»‹. ÄÃ¢y láº¡i lÃ  hÃ¬nh áº£nh cá»§a Ä‘áº¡o hÃ m riÃªng ğŸ™‚


ÄÃ¡nh giÃ¡ vá» Ä‘áº¡o hÃ m riÃªng trong Ä‘á»“ thá»‹, chÃºng ta cáº§n tá»›i quy táº¯c tá»•ng vÃ  quy táº¯c nhÃ¢n. NghÄ©a lÃ  quy táº¯c tá»•ng vá»›i táº¥t cáº£ cÃ¡c Ä‘Æ°á»ng cÃ³ thá»ƒ tá»« 1 node nÃ o Ä‘Ã³, vÃ  quy táº¯c nhÃ¢n vá»›i Ä‘áº¡o hÃ m vá»›i má»—i Ä‘Æ°á»ng dáº«n vÃ o nhau<br/>
VÃ­ dá»¥ muá»‘n Ä‘Ã¡nh giÃ¡ b áº£nh hÆ°á»Ÿng nhÆ° tháº¿ nÃ o Ä‘á»‘i vá»›i e ta lÃ m nhÆ° sau: <br/>

e cÃ³ 2 ngÃ£ vÃ o bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi b. Giá» xuáº¥t phÃ¡t tá»« b ta nhÃ¢n cho Ä‘áº¿n khi gáº§n tá»›i e, rá»“i sau Ä‘Ã³ tá»•ng cÃ¡c hÆ°á»›ng Ä‘Ã£ nhÃ¢n.

      âˆ‚e/âˆ‚b = 1 * 2 + 1 * 3 = 5

> Váº­y b thay Ä‘á»•i 1 Ä‘Æ¡n vá»‹ thÃ¬ e sáº½ thay Ä‘á»•i xáº¥p xá»‰ trÃªn dÆ°á»›i 5 Ä‘Æ¡n vá»‹. ÄÃ³ chÃ­nh lÃ  má»™t pháº§n Ã½ nghÄ©a cá»§a Ä‘áº¡o hÃ m riÃªng. Báº¡n hÃ£y bá» tÃºi skill nÃ y nhÃ©!!

Äáº¿n Ä‘Ã¢y cháº¯c háº³n cÃ³ nhiá»u báº¡n cÅ©ng chÆ°a hiá»ƒu **Computational Graph náº¿u Ã¡p dá»¥ng vÃ o network** nhÆ° tháº¿ nÃ o?
Má»¥c Ä‘Ã­ch cá»§a ta lÃ  thay Ä‘á»•i hay tÃ¡c Ä‘á»™ng Ä‘áº¿n cÃ¡c trá»ng sá»‘ W Ä‘á»ƒ lÃ m cho L thay Ä‘á»•i vÃ  thay Ä‘á»•i vá» giÃ¡ trá»‹ nhá» nháº¥t. á» Ä‘Ã¢y ngÆ°á»i ta dÃ¹ng phÆ°Æ¡ng phÃ¡p Reverse-mode differentiation thay vÃ¬ Forward-mode differentiation trong Ä‘áº¡o hÃ m riÃªng. Hoáº·c nÃ³i má»™t cÃ¡i tÃªn thÃ¢n thÆ°Æ¡ng lÃ : Backpropagation. 

VÃ¬ sao váº­y? ÄÃ¡p Ã¡n ngay dÆ°á»›i Ä‘Ã¢y ğŸ™‚

**Forward-mode differentiation** tá»« b lÃªn. Äiá»u nÃ y cho chÃºng ta Ä‘áº¡o hÃ m táº¡i má»—i node vá»›i sá»± tÃ¡c Ä‘á»™ng cá»§a **b vÃ  chá»‰ b**

![](https://i.imgur.com/3j2x8yy.png)


**Reverse-mode differentiation** tá»« e xuá»‘ng? Äiá»u nÃ y cho chÃºng ta Ä‘áº¡o hÃ m cá»§a e vá»›i sá»± tÃ¡c Ä‘á»™ng tá»›i má»—i node vá»›i sá»± tÃ¡c Ä‘á»™ng cá»§a a vÃ  b. Ã tÃ´i lÃ  nháº­n Ä‘Æ°á»£c cáº£ âˆ‚e/âˆ‚a vÃ  âˆ‚e/âˆ‚b => QuÃ¡ Ä‘Ã£

![](https://i.imgur.com/375b9Yj.png)


Káº¿t luáº­n: 

> Reverse-mode differentiation thá»±c hiá»‡n Ä‘áº¡o hÃ m cá»§a e Ä‘á»‘i vá»›i cÃ¡c node vÃ  ta nháº­n Ä‘Æ°á»£c cáº£ âˆ‚e/âˆ‚a vÃ  âˆ‚e/âˆ‚b<br/>
Trong khi Ä‘Ã³ Forward-mode differentiation Ä‘Ã£ cho chÃºng ta Ä‘áº¡o hÃ m cá»§a output vá»›i sá»± tÃ¡c Ä‘á»™ng cá»§a **chá»‰ 1 input**, nhÆ°ng reverse-mode differentiation cho chÃºng ta táº¥t cáº£ chÃºng.
Ta thá»­ tÆ°á»Ÿng tÆ°á»£ng 1 hÃ m vá»›i 1 triá»‡u input vÃ  1 output. Forward-mode differentiation sáº½ yÃªu cáº§u nÃ³ quáº¹o vÃ²ng qua Ä‘á»“ thá»‹ 1 triá»‡u láº§n Ä‘á»ƒ nháº­n Ä‘Æ°á»£c cÃ¡c Ä‘áº¡o hÃ m. Reverse-mode differentiation cÃ³ thá»ƒ nháº­n táº¥t cáº£ chÃºng chá»‰ vá»›i 1 láº§n. @@

Computational Graph giÃºp cÃ³ thá»ƒ dá»… dÃ ng minh há»a trá»±c quan tÃ­nh Ä‘áº¡o hÃ m cho cÃ¡c biáº¿n vÃ  Ä‘á»ƒ hiá»ƒu sÃ¢u vá» neural network, hay xa hÆ¡n lÃ  cÃ¡c mÃ´ hÃ¬nh deep learning, thÃ¬ viá»‡c hiá»ƒu rÃµ Computational Graph nÃ y mang láº¡i ráº¥t nhiá»u há»¯u Ã­ch cho cÃ¡c báº¡n.

Tham kháº£o: 

1.   https://aivietnam.ai/courses/aisummer2019/lessons/computational-graph/
2.   https://colah.github.io/posts/2015-08-Backprop/





# **2.LinearRegression**
> Write by Quang Van

### **3.BÃ€I Táº¬P**

#### **Class LinearRegresstion**
sá»­ dá»¥ng cho cÃ¡c bÃ i toÃ¡n


```python
# -*- coding: utf-8 -*-
"""
Created on Sat Sep 12 11:28:25 2020

@author: Admin
"""
import numpy as np
import matplotlib.pyplot as plt

class LinearRegresstion:
    """
        propety: 
            path: path of file contain data
            rate: leanring rate
            epochs = 100: Number of loop
            miniBatchSize = None: size of mini Data
        method:
            run(): run LinearRegresstion
            showLossGraph: show graph of losses
                
    """
    
    def __init__(self, data, rate, epochs = 100, miniBatchSize = None):
        self.data, self.coefficient = self.getNomal(data[:].copy())
        self.features = self.data[:, :-1]
        self.y = self.data[:, -1:]
        self.rate = rate
        self.epochs = epochs
        self.w = np.random.rand(self.features.shape[1], self.y.shape[1])
        self.b = np.random.rand(1)
        self.nData = self.features.shape[0]
        self.miniBatchSize = miniBatchSize
        self.losses = []

    def getNomal(self, data):
        maxi = np.max(data, axis = 0)
        mini = np.min(data, axis = 0)
        avg = np.mean(data, axis = 0)
        return (data-avg) / (maxi-mini), (maxi, mini, avg)
    
    def run(self):
        if (self.miniBatchSize == None):
            self.miniBatchSize = self.nData
            
        for epoch in range(self.epochs):

                iIndex = np.random.randint(0, self.nData, size=(self.miniBatchSize,))
                xi = self.features[iIndex]
                yi = self.y[iIndex]
                
                predict = self.features[iIndex].dot(self.w) + self.b
                
                loss = 1/2 * (predict - yi)**2
                self.losses.append(loss.mean())

                loss_grd = (predict - yi)/self.miniBatchSize
                self.w -= self.rate * xi.T.dot(loss_grd)
                self.b -= self.rate * np.sum(loss_grd)
                
    def showLossGraph(self):
        plt.plot(self.losses, 'r')
        plt.show()

def loadFile(path):
    return np.genfromtxt(path, delimiter=',', skip_header=1)

```

#### **Dataset**: [Here](https://drive.google.com/drive/folders/1y8eOn4c5rwhCQWREyuktnaINfIIvah4m?usp=sharing)


#### **a) BostonHousing**


```python
data = loadFile("/content/drive/My Drive/Colab Notebooks/AI_Zumpzero_lv1/dataset/linearRegresstion/BostonHousing.csv")
myObject = LinearRegresstion(data, 1.5, epochs = 100, miniBatchSize = 64)
myObject.run()
myObject.showLossGraph()
```


![png](output_9_0.png)


#### **b) Advertising**


```python
data = loadFile("/content/drive/My Drive/Colab Notebooks/AI_Zumpzero_lv1/dataset/linearRegresstion/advertising.csv")
myObject = LinearRegresstion(data, 1.5, epochs = 100, miniBatchSize = 64)
myObject.run()
myObject.showLossGraph()
```


![png](output_11_0.png)


#### **c) GiÃ¡ nhÃ **


```python
data = loadFile("/content/drive/My Drive/Colab Notebooks/AI_Zumpzero_lv1/dataset/linearRegresstion/pricebyarea.csv")
myObject = LinearRegresstion(data, 1.5, epochs = 100, miniBatchSize = 64)
myObject.run()
myObject.showLossGraph()
```


![png](output_13_0.png)

